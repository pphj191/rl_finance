# Reward Function Design Guide

> **ì‘ì„±ì¼**: 2025-10-07
> **ëª©ì **: RL ì—ì´ì „íŠ¸ì˜ ë¦¬ì›Œë“œ í•¨ìˆ˜ ì„¤ê³„ ë° ë¹„êµ ë¶„ì„

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ë¦¬ì›Œë“œ ì‹œìŠ¤í…œì˜ ë¬¸ì œì ](#í˜„ì¬-ë¦¬ì›Œë“œ-ì‹œìŠ¤í…œì˜-ë¬¸ì œì )
2. [ë¦¬ì›Œë“œ ì„¤ê³„ ì›ì¹™](#ë¦¬ì›Œë“œ-ì„¤ê³„-ì›ì¹™)
3. [ì œì•ˆëœ ë¦¬ì›Œë“œ í•¨ìˆ˜ë“¤](#ì œì•ˆëœ-ë¦¬ì›Œë“œ-í•¨ìˆ˜ë“¤)
4. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)
5. [ì‹¤í—˜ ë° í‰ê°€](#ì‹¤í—˜-ë°-í‰ê°€)

---

## í˜„ì¬ ë¦¬ì›Œë“œ ì‹œìŠ¤í…œì˜ ë¬¸ì œì 

### í˜„ì¬ êµ¬í˜„ (rl_env.py:262-269)
```python
# í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”ìœ¨
prev_total_value = self.total_value
# ... ì•¡ì…˜ ì‹¤í–‰ ...
self.total_value = self.balance + self.position * current_price

if prev_total_value > 0:
    reward = (self.total_value - prev_total_value) / prev_total_value
else:
    reward = (self.total_value - prev_total_value) / self.config.initial_balance
```

### ë¬¸ì œì  ë¶„ì„

#### 1. **ë§¤ë„ íšŒí”¼ ë¬¸ì œ (Sell Aversion)**
- **í˜„ìƒ**: ì—ì´ì „íŠ¸ê°€ ë§¤ë„ë¥¼ ê±°ì˜ í•˜ì§€ ì•ŠìŒ
- **ì›ì¸**:
  - ìˆ˜ìˆ˜ë£Œ(0.05%)ë¡œ ì¸í•œ ì¦‰ê°ì ì¸ ì†ì‹¤
  - ë§¤ë„ í›„ ê¸°íšŒë¹„ìš©ì— ëŒ€í•œ ê³ ë ¤ ì—†ìŒ
  - ê°€ê²© ìƒìŠ¹ ì‹œ ë³´ìœ ë§Œìœ¼ë¡œë„ ì–‘ì˜ ë¦¬ì›Œë“œ íšë“

**ì˜ˆì‹œ**:
```
BTC ê°€ê²©: 50,000,000ì›
ë³´ìœ  ì¤‘: 0.01 BTC

# ìŠ¤í… N: HOLD
ê°€ê²© 1% ìƒìŠ¹ â†’ 50,500,000ì›
total_value: 505,000 â†’ 510,050
reward = (510,050 - 505,000) / 505,000 = 0.01 (1% ë¦¬ì›Œë“œ)

# ìŠ¤í… N+1: SELL ê³ ë ¤
ë§¤ë„ ì‹œ: ìˆ˜ìˆ˜ë£Œ 0.05%
balance = 0.01 * 50,500,000 * 0.9995 = 504,747
reward = (504,747 - 510,050) / 510,050 = -0.0104 (-1.04% ë¦¬ì›Œë“œ)

â†’ ë§¤ë„í•˜ë©´ ë§ˆì´ë„ˆìŠ¤ ë¦¬ì›Œë“œ! ë§¤ë„ ì•ˆ í•˜ëŠ”ê²Œ ì´ë“
```

#### 2. **ë‹¨ê¸° ê°€ê²© ë³€ë™ì— ê³¼ë„í•œ ë¯¼ê°ì„±**
- ë§¤ ìŠ¤í…ë§ˆë‹¤ ê°€ê²© ë³€í™”ë§Œ ë°˜ì˜
- ì¥ê¸° íŠ¸ë Œë“œ ë¬´ì‹œ
- ë…¸ì´ì¦ˆì— ì·¨ì•½

#### 3. **ë¦¬ìŠ¤í¬ ë¬´ì‹œ**
- ë³€ë™ì„±ì— ëŒ€í•œ í˜ë„í‹° ì—†ìŒ
- ìµœëŒ€ ë‚™í­(MDD) ê³ ë ¤ ì•ˆ í•¨
- ê³¼ë„í•œ ê±°ë˜ì— ëŒ€í•œ í˜ë„í‹° ì—†ìŒ

#### 4. **í¬ì†Œ ë¦¬ì›Œë“œ (Sparse Reward)**
- HOLD ì‹œ ê°€ê²© ë³€ë™ë§Œ ë¦¬ì›Œë“œ
- BUY/SELLì˜ ì§ˆ(quality) í‰ê°€ ì—†ìŒ
- íƒ€ì´ë° ì¢‹ì€ ê±°ë˜ì— ëŒ€í•œ ë³´ìƒ ë¶€ì¡±

---

## ë¦¬ì›Œë“œ ì„¤ê³„ ì›ì¹™

### 1. **ë§¤ë„ ì¸ì„¼í‹°ë¸Œ (Sell Incentive)**
âœ… ìˆ˜ìµ ì‹¤í˜„ ì‹œ ë³´ë„ˆìŠ¤ ì œê³µ
âœ… ì ì ˆí•œ ì†ì ˆì— ëŒ€í•œ ìµœì†Œ í˜ë„í‹°
âœ… ê³¼ë„í•œ ë³´ìœ ì— ëŒ€í•œ í˜ë„í‹°

### 2. **ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  (Risk-Adjusted Returns)**
âœ… ë³€ë™ì„± ê³ ë ¤
âœ… MDD í˜ë„í‹°
âœ… ìƒ¤í”„ ë¹„ìœ¨ ê¸°ë°˜ ë³´ìƒ

### 3. **í–‰ë™ í’ˆì§ˆ í‰ê°€ (Action Quality)**
âœ… ì¢‹ì€ íƒ€ì´ë° ë§¤ìˆ˜/ë§¤ë„ ë³´ë„ˆìŠ¤
âœ… ë‚˜ìœ íƒ€ì´ë° í˜ë„í‹°
âœ… ê³¼ë„í•œ ê±°ë˜ ì–µì œ

### 4. **ì¥ê¸° ëª©í‘œ ì •ë ¬ (Long-term Alignment)**
âœ… ì—í”¼ì†Œë“œ ì „ì²´ ìˆ˜ìµë¥  ì¤‘ì‹œ
âœ… ë‹¨ê¸° ë…¸ì´ì¦ˆ ë¬´ì‹œ
âœ… ì¼ê´€ëœ ì „ëµ ìœ ë„

---

## ì œì•ˆëœ ë¦¬ì›Œë“œ í•¨ìˆ˜ë“¤

### ë¦¬ì›Œë“œ 1: ë§¤ë„ ì¸ì„¼í‹°ë¸Œ ì¶”ê°€ (Sell Incentive)

```python
def reward_with_sell_incentive(self, action, prev_total_value, current_total_value):
    """
    ê¸°ë³¸ ìˆ˜ìµë¥  + ë§¤ë„ ì¸ì„¼í‹°ë¸Œ
    """
    # ê¸°ë³¸ ìˆ˜ìµë¥ 
    base_reward = (current_total_value - prev_total_value) / prev_total_value

    # ë§¤ë„ ì¸ì„¼í‹°ë¸Œ
    if action == ActionSpace.SELL and self.last_buy_price > 0:
        # ìˆ˜ìµ ì‹¤í˜„ ë³´ë„ˆìŠ¤
        profit_rate = (self.sell_price - self.last_buy_price) / self.last_buy_price

        if profit_rate > 0:
            # ìˆ˜ìµ ë§¤ë„: í° ë³´ë„ˆìŠ¤
            sell_bonus = profit_rate * 0.5  # ìˆ˜ìµì˜ 50%ë¥¼ ì¶”ê°€ ë¦¬ì›Œë“œ
        elif profit_rate > -0.02:
            # ì†Œì•¡ ì†ì‹¤ ë§¤ë„: ì‘ì€ ë³´ë„ˆìŠ¤ (ë¹ ë¥¸ ì†ì ˆ ì¥ë ¤)
            sell_bonus = 0.005
        else:
            # í° ì†ì‹¤ ë§¤ë„: ì‘ì€ í˜ë„í‹°
            sell_bonus = profit_rate * 0.1

        base_reward += sell_bonus

    # ì¥ê¸° ë³´ìœ  í˜ë„í‹° (30 ìŠ¤í… ì´ìƒ ë³´ìœ  ì‹œ)
    if self.position > 0 and self.hold_duration > 30:
        hold_penalty = -0.0001 * self.hold_duration
        base_reward += hold_penalty

    return base_reward
```

**ì¥ì **:
- âœ… ë§¤ë„ ì‹œ ìˆ˜ìµ ì‹¤í˜„ ë³´ë„ˆìŠ¤
- âœ… ì†ì ˆ ì‹œ ìµœì†Œ í˜ë„í‹°
- âœ… ê³¼ë„í•œ ë³´ìœ  ì–µì œ

**ë‹¨ì **:
- âš ï¸ íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
- âš ï¸ ì§§ì€ ì—í”¼ì†Œë“œì—ì„œëŠ” íš¨ê³¼ ì œí•œì 

---

### ë¦¬ì›Œë“œ 2: ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  (Risk-Adjusted Return)

```python
def reward_risk_adjusted(self, action, prev_total_value, current_total_value, window=20):
    """
    ìƒ¤í”„ ë¹„ìœ¨ ê¸°ë°˜ ë¦¬ì›Œë“œ
    """
    # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ 
    portfolio_return = (current_total_value - prev_total_value) / prev_total_value

    # ìµœê·¼ ìˆ˜ìµë¥  ë³€ë™ì„± ê³„ì‚°
    self.recent_returns.append(portfolio_return)
    if len(self.recent_returns) > window:
        self.recent_returns.pop(0)

    if len(self.recent_returns) >= 2:
        returns_std = np.std(self.recent_returns)
        # ìƒ¤í”„ ë¹„ìœ¨ ìŠ¤íƒ€ì¼ ë¦¬ì›Œë“œ (ë¬´ìœ„í—˜ ìˆ˜ìµë¥  = 0)
        reward = portfolio_return / (returns_std + 1e-6)
    else:
        reward = portfolio_return

    # MDD í˜ë„í‹°
    if current_total_value < self.peak_value:
        drawdown = (self.peak_value - current_total_value) / self.peak_value
        reward -= drawdown * 0.5  # MDD í˜ë„í‹°
    else:
        self.peak_value = current_total_value

    return reward
```

**ì¥ì **:
- âœ… ë³€ë™ì„± ë‚®ì€ ì•ˆì •ì  ìˆ˜ìµ ì„ í˜¸
- âœ… ìµœëŒ€ ë‚™í­ ìµœì†Œí™”
- âœ… ìœ„í—˜ ê´€ë¦¬ í•™ìŠµ

**ë‹¨ì **:
- âš ï¸ ì´ˆê¸° í•™ìŠµ ë¶ˆì•ˆì •
- âš ï¸ ê³„ì‚° ë³µì¡ë„ ì¦ê°€

---

### ë¦¬ì›Œë“œ 3: ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì´ˆê³¼ ìˆ˜ìµ (Benchmark Excess Return)

```python
def reward_benchmark_excess(self, action, prev_total_value, current_total_value):
    """
    Buy & Hold ëŒ€ë¹„ ì´ˆê³¼ ìˆ˜ìµ
    """
    # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ 
    portfolio_return = (current_total_value - prev_total_value) / prev_total_value

    # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  (ë‹¨ìˆœ ë³´ìœ )
    benchmark_return = (self.current_price - self.prev_price) / self.prev_price

    # ì´ˆê³¼ ìˆ˜ìµ
    excess_return = portfolio_return - benchmark_return

    return excess_return
```

**ì¥ì **:
- âœ… ë‹¨ìˆœ ë³´ìœ ë³´ë‹¤ ë‚˜ì€ ì „ëµ í•™ìŠµ
- âœ… ì ˆëŒ€ ìˆ˜ìµì´ ì•„ë‹Œ ìƒëŒ€ ì„±ê³¼ ì¤‘ì‹œ
- âœ… ì‹œì¥ ìƒí™© ë¬´ê´€í•˜ê²Œ í•™ìŠµ

**ë‹¨ì **:
- âš ï¸ ìŒì˜ ë¦¬ì›Œë“œ ë¹ˆë²ˆ (ì´ˆê¸° í•™ìŠµ ì–´ë ¤ì›€)
- âš ï¸ ë²¤ì¹˜ë§ˆí¬ ì„ íƒì— ë”°ë¼ ê²°ê³¼ ë³€ë™

---

### ë¦¬ì›Œë“œ 4: í–‰ë™ í’ˆì§ˆ ê¸°ë°˜ (Action Quality-Based)

```python
def reward_action_quality(self, action, prev_total_value, current_total_value):
    """
    ë§¤ìˆ˜/ë§¤ë„ íƒ€ì´ë° í’ˆì§ˆ í‰ê°€
    """
    base_reward = (current_total_value - prev_total_value) / prev_total_value

    # ê°€ê²© ì¶”ì„¸ ê³„ì‚° (ìµœê·¼ 5ìŠ¤í… ê¸°ìš¸ê¸°)
    price_trend = (self.current_price - self.price_5_steps_ago) / self.price_5_steps_ago

    if action == ActionSpace.BUY:
        if price_trend < -0.01:  # í•˜ë½ ì¶”ì„¸ì—ì„œ ë§¤ìˆ˜ â†’ ì¢‹ì€ íƒ€ì´ë°
            base_reward += 0.01
        elif price_trend > 0.02:  # ê¸‰ë“± í›„ ë§¤ìˆ˜ â†’ ë‚˜ìœ íƒ€ì´ë°
            base_reward -= 0.01

    elif action == ActionSpace.SELL:
        if price_trend > 0.02:  # ìƒìŠ¹ ì¶”ì„¸ì—ì„œ ë§¤ë„ â†’ ì¢‹ì€ íƒ€ì´ë°
            base_reward += 0.01
        elif price_trend < -0.01:  # í•˜ë½ ì¶”ì„¸ì—ì„œ ë§¤ë„ â†’ ëŠ¦ì€ ë§¤ë„
            base_reward -= 0.005

    # ê³¼ë„í•œ ê±°ë˜ í˜ë„í‹°
    self.trade_count += (action != ActionSpace.HOLD)
    if self.trade_count > 50:  # 50íšŒ ì´ìƒ ê±°ë˜ ì‹œ
        base_reward -= 0.0001 * self.trade_count

    return base_reward
```

**ì¥ì **:
- âœ… ì¢‹ì€ íƒ€ì´ë° í•™ìŠµ
- âœ… ê³¼ë„í•œ ê±°ë˜ ì–µì œ
- âœ… í•´ì„ ê°€ëŠ¥ì„±

**ë‹¨ì **:
- âš ï¸ ì¶”ì„¸ íŒë‹¨ ë¡œì§ í•„ìš”
- âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë§ìŒ

---

### ë¦¬ì›Œë“œ 5: ë³µí•© ë¦¬ì›Œë“œ (Hybrid Reward)

```python
def reward_hybrid(self, action, prev_total_value, current_total_value):
    """
    ì—¬ëŸ¬ ë¦¬ì›Œë“œ ìš”ì†Œë¥¼ ê°€ì¤‘ ê²°í•©
    """
    # 1. ê¸°ë³¸ ìˆ˜ìµë¥  (ê°€ì¤‘ì¹˜: 0.5)
    base_reward = (current_total_value - prev_total_value) / prev_total_value

    # 2. ë§¤ë„ ì¸ì„¼í‹°ë¸Œ (ê°€ì¤‘ì¹˜: 0.2)
    sell_incentive = self._calculate_sell_incentive(action)

    # 3. ìœ„í—˜ ì¡°ì • (ê°€ì¤‘ì¹˜: 0.2)
    risk_penalty = self._calculate_risk_penalty()

    # 4. í–‰ë™ í’ˆì§ˆ (ê°€ì¤‘ì¹˜: 0.1)
    action_quality = self._calculate_action_quality(action)

    # ê°€ì¤‘ ê²°í•©
    reward = (
        0.5 * base_reward +
        0.2 * sell_incentive +
        0.2 * risk_penalty +
        0.1 * action_quality
    )

    return reward
```

**ì¥ì **:
- âœ… ë‹¤ì–‘í•œ ëª©í‘œ ê· í˜•
- âœ… ê°€ì¤‘ì¹˜ë¡œ ì¡°ì ˆ ê°€ëŠ¥
- âœ… ì•ˆì •ì  í•™ìŠµ

**ë‹¨ì **:
- âš ï¸ ë³µì¡ë„ ì¦ê°€
- âš ï¸ ê°€ì¤‘ì¹˜ íŠœë‹ í•„ìš”

---

### ë¦¬ì›Œë“œ 6: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œì  ë³´ìƒ (Terminal Reward)

```python
def reward_terminal(self, action, prev_total_value, current_total_value, done):
    """
    ë§¤ ìŠ¤í…: ì‘ì€ ë¦¬ì›Œë“œ
    ì—í”¼ì†Œë“œ ì¢…ë£Œ: í° ë¦¬ì›Œë“œ
    """
    # ìŠ¤í… ë¦¬ì›Œë“œ (0.1 ê°€ì¤‘ì¹˜)
    step_reward = (current_total_value - prev_total_value) / prev_total_value * 0.1

    # ì—í”¼ì†Œë“œ ì¢…ë£Œ ë¦¬ì›Œë“œ (10ë°° ê°€ì¤‘ì¹˜)
    terminal_reward = 0
    if done:
        final_return = (current_total_value - self.initial_balance) / self.initial_balance
        terminal_reward = final_return * 10

        # ê±°ë˜ íšŸìˆ˜ ë³´ë„ˆìŠ¤/í˜ë„í‹°
        if self.trade_count < 5:
            terminal_reward -= 0.1  # ë„ˆë¬´ ì ì€ ê±°ë˜
        elif self.trade_count > 100:
            terminal_reward -= 0.1  # ë„ˆë¬´ ë§ì€ ê±°ë˜

    return step_reward + terminal_reward
```

**ì¥ì **:
- âœ… ì¥ê¸° ëª©í‘œ ì§‘ì¤‘
- âœ… ë‹¨ê¸° ë…¸ì´ì¦ˆ ë¬´ì‹œ
- âœ… ì—í”¼ì†Œë“œ ì „ì²´ ìµœì í™”

**ë‹¨ì **:
- âš ï¸ í•™ìŠµ ì´ˆê¸° ëŠë¦¼ (í¬ì†Œ ë¦¬ì›Œë“œ)
- âš ï¸ í¬ë ˆë”§ í• ë‹¹ ë¬¸ì œ

---

## êµ¬í˜„ ê³„íš

### Phase 1: ë¦¬ì›Œë“œ í•¨ìˆ˜ ëª¨ë“ˆ ìƒì„±
- [ ] `trading_env/reward_functions.py` ìƒì„±
- [ ] 6ê°€ì§€ ë¦¬ì›Œë“œ í•¨ìˆ˜ êµ¬í˜„
- [ ] ë¦¬ì›Œë“œ ì„ íƒ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

```python
# reward_functions.py
class RewardFunction:
    def __init__(self, reward_type='basic'):
        self.reward_type = reward_type

    def calculate(self, env, action, prev_total_value):
        if self.reward_type == 'basic':
            return self.reward_basic(...)
        elif self.reward_type == 'sell_incentive':
            return self.reward_with_sell_incentive(...)
        # ...
```

### Phase 2: í™˜ê²½ í†µí•©
- [ ] `rl_env.py` ìˆ˜ì •
- [ ] ë¦¬ì›Œë“œ í•¨ìˆ˜ë¥¼ configë¡œ ì„ íƒ ê°€ëŠ¥í•˜ê²Œ
- [ ] í•„ìš”í•œ ìƒíƒœ ë³€ìˆ˜ ì¶”ê°€ (hold_duration, trade_count ë“±)

```python
# TradingConfig
@dataclass
class TradingConfig:
    # ...
    reward_type: str = "basic"  # 'basic', 'sell_incentive', 'risk_adjusted', ...
```

### Phase 3: ì‹¤í—˜ ë° ë¹„êµ
- [ ] ê° ë¦¬ì›Œë“œ í•¨ìˆ˜ë³„ í•™ìŠµ ì‹¤í–‰
- [ ] ì„±ê³¼ ì§€í‘œ ë¹„êµ:
  - ìµœì¢… ìˆ˜ìµë¥ 
  - ìƒ¤í”„ ë¹„ìœ¨
  - ìµœëŒ€ ë‚™í­
  - ë§¤ìˆ˜/ë§¤ë„ ë¹ˆë„
  - ìŠ¹ë¥ 
- [ ] ê²°ê³¼ ì‹œê°í™” ë° ë³´ê³ ì„œ ì‘ì„±

### Phase 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ìµœì  ë¦¬ì›Œë“œ í•¨ìˆ˜ ì„ ì •
- [ ] ê°€ì¤‘ì¹˜/ì„ê³„ê°’ íŠœë‹
- [ ] êµì°¨ ê²€ì¦

---

## ì‹¤í—˜ ë° í‰ê°€

### ì‹¤í—˜ ì„¤ì •
```python
# ê³µí†µ ì„¤ì •
config = TradingConfig(
    initial_balance=1000000,
    lookback_window=60,
    model_type="dqn",
    learning_rate=1e-4,
    batch_size=32,
    epsilon_decay=0.995
)

# ë¦¬ì›Œë“œ í•¨ìˆ˜ë³„ í•™ìŠµ
reward_types = [
    'basic',
    'sell_incentive',
    'risk_adjusted',
    'benchmark_excess',
    'action_quality',
    'hybrid',
    'terminal'
]

for reward_type in reward_types:
    config.reward_type = reward_type
    trainer = TradingTrainer(config, market="KRW-BTC")
    results = trainer.train(num_episodes=1000)
    # ê²°ê³¼ ì €ì¥ ë° ë¹„êµ
```

### í‰ê°€ ì§€í‘œ

| ì§€í‘œ | ì„¤ëª… | ëª©í‘œ |
|-----|------|-----|
| **ì´ ìˆ˜ìµë¥ ** | (ìµœì¢… ìë³¸ - ì´ˆê¸° ìë³¸) / ì´ˆê¸° ìë³¸ | ìµœëŒ€í™” |
| **ìƒ¤í”„ ë¹„ìœ¨** | í‰ê·  ìˆ˜ìµ / ë³€ë™ì„± | ìµœëŒ€í™” |
| **ìµœëŒ€ ë‚™í­** | ìµœê³ ì  ëŒ€ë¹„ ìµœëŒ€ í•˜ë½í­ | ìµœì†Œí™” |
| **ë§¤ìˆ˜ íšŸìˆ˜** | ì—í”¼ì†Œë“œë‹¹ í‰ê·  ë§¤ìˆ˜ íšŸìˆ˜ | ì ì • ìˆ˜ì¤€ |
| **ë§¤ë„ íšŸìˆ˜** | ì—í”¼ì†Œë“œë‹¹ í‰ê·  ë§¤ë„ íšŸìˆ˜ | ì ì • ìˆ˜ì¤€ (>0) |
| **ìŠ¹ë¥ ** | ìˆ˜ìµ ê±°ë˜ / ì „ì²´ ê±°ë˜ | >50% |
| **í•™ìŠµ ì•ˆì •ì„±** | ë¦¬ì›Œë“œ ë¶„ì‚° | ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ |

### ì˜ˆìƒ ê²°ê³¼

| ë¦¬ì›Œë“œ í•¨ìˆ˜ | ìˆ˜ìµë¥  ì˜ˆìƒ | ë§¤ë„ ë¹ˆë„ ì˜ˆìƒ | í•™ìŠµ ë‚œì´ë„ |
|-----------|----------|------------|-----------|
| Basic | ì¤‘ê°„ | ë‚®ìŒ âš ï¸ | ì‰¬ì›€ |
| Sell Incentive | ë†’ìŒ | ë†’ìŒ âœ… | ì‰¬ì›€ |
| Risk-Adjusted | ì¤‘ê°„ | ì¤‘ê°„ | ì–´ë ¤ì›€ |
| Benchmark Excess | ë†’ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| Action Quality | ë†’ìŒ | ì¤‘ê°„ | ì¤‘ê°„ |
| Hybrid | ë†’ìŒ | ë†’ìŒ âœ… | ì¤‘ê°„ |
| Terminal | ë†’ìŒ | ì¤‘ê°„ | ì–´ë ¤ì›€ |

---

## ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰
1. [ ] `reward_functions.py` êµ¬í˜„
2. [ ] `rl_env.py` í†µí•©
3. [ ] ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (10 ì—í”¼ì†Œë“œ)

### ë‹¨ê¸° (1ì£¼ì¼)
4. [ ] ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ (1000 ì—í”¼ì†Œë“œ)
5. [ ] ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
6. [ ] ìµœì  ë¦¬ì›Œë“œ í•¨ìˆ˜ ì„ ì •

### ì¥ê¸° (1ê°œì›”)
7. [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
8. [ ] ë‹¤ì–‘í•œ ì‹œì¥ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
9. [ ] ë…¼ë¬¸ ì‘ì„± (ì„ íƒ)

---

**ì‘ì„±ì ë…¸íŠ¸**:
ì´ ë¬¸ì„œëŠ” ë¦¬ì›Œë“œ ì„¤ê³„ì˜ ì²­ì‚¬ì§„ì…ë‹ˆë‹¤. ì‹¤ì œ êµ¬í˜„ ë° ì‹¤í—˜ì„ í†µí•´ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë  ì˜ˆì •ì…ë‹ˆë‹¤.
