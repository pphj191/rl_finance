"""
SQLite ê¸°ë°˜ ì‹œì¥ ë°ì´í„° ì €ì¥ ë° ë¡œë“œ

íŠ¸ë ˆì´ë”© ë°ì´í„°ë¥¼ SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
"""

import sqlite3
import pandas as pd
import numpy as np
import pickle
import json
import hashlib
from typing import Optional, List, Dict, Tuple, Any
from datetime import datetime, timedelta
import logging
from pathlib import Path


def align_timestamp(dt, timeframe: str) -> datetime:
    """
    íƒ€ì„í”„ë ˆì„ì— ë§ê²Œ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ê·œí™” (ì´ˆ/ë°€ë¦¬ì´ˆ ì œê±°)

    Args:
        dt: datetime ê°ì²´, pandas Timestamp, ë˜ëŠ” Unix timestamp (int/float)
        timeframe: '1m', '1h', '1d'

    Returns:
        ì •ê·œí™”ëœ datetime

    Examples:
        - 1m: 2025-10-09 14:23:45.123 â†’ 2025-10-09 14:23:00
        - 1h: 2025-10-09 14:23:45.123 â†’ 2025-10-09 14:00:00
        - 1d: 2025-10-09 14:23:45.123 â†’ 2025-10-09 00:00:00
    """
    # pandas Timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    if hasattr(dt, 'to_pydatetime'):
        dt = dt.to_pydatetime()
    # Unix timestamp (int ë˜ëŠ” float)ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    elif isinstance(dt, (int, float)):
        dt = datetime.fromtimestamp(dt)

    if timeframe == '1m':
        return dt.replace(second=0, microsecond=0)
    elif timeframe == '1h':
        return dt.replace(minute=0, second=0, microsecond=0)
    elif timeframe == '1d':
        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")


def get_missing_ranges(
    storage: 'MarketDataStorage',
    market: str,
    timeframe: str,
    target_start: datetime,
    target_end: datetime
) -> List[Tuple[datetime, datetime]]:
    """
    ìˆ˜ì§‘í•´ì•¼ í•  ëˆ„ë½ ë°ì´í„° ë²”ìœ„ ê³„ì‚°

    Args:
        storage: MarketDataStorage ì¸ìŠ¤í„´ìŠ¤
        market: ë§ˆì¼“ ì½”ë“œ
        timeframe: '1m', '1h', '1d'
        target_start: ëª©í‘œ ì‹œì‘ ì‹œê°„
        target_end: ëª©í‘œ ì¢…ë£Œ ì‹œê°„

    Returns:
        [(ì‹œì‘1, ì¢…ë£Œ1), (ì‹œì‘2, ì¢…ë£Œ2), ...] ëˆ„ë½ëœ ì‹œê°„ ë²”ìœ„ ë¦¬ìŠ¤íŠ¸
    """
    logger = logging.getLogger(__name__)

    # 1. ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ
    existing_start, existing_end = storage.get_data_range_by_timeframe(market, timeframe)

    # 2. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ êµ¬ê°„ ìˆ˜ì§‘
    if existing_start is None:
        logger.info(f"  [{timeframe}] ê¸°ì¡´ ë°ì´í„° ì—†ìŒ â†’ ì „ì²´ êµ¬ê°„ ìˆ˜ì§‘")
        return [(target_start, target_end)]

    logger.info(f"  [{timeframe}] ê¸°ì¡´ ë°ì´í„°: {existing_start} ~ {existing_end}")

    # 3. ëˆ„ë½ êµ¬ê°„ ê³„ì‚°
    missing_ranges = []

    # ì•ë¶€ë¶„ ëˆ„ë½ (target_start < existing_start)
    if target_start < existing_start:
        missing_ranges.append((target_start, existing_start))
        logger.info(f"  [{timeframe}] ì•ë¶€ë¶„ ëˆ„ë½: {target_start} ~ {existing_start}")

    # ë’·ë¶€ë¶„ ëˆ„ë½ (target_end > existing_end)
    if target_end > existing_end:
        missing_ranges.append((existing_end, target_end))
        logger.info(f"  [{timeframe}] ë’·ë¶€ë¶„ ëˆ„ë½: {existing_end} ~ {target_end}")

    # TODO: ì¤‘ê°„ êµ¬ê°„ ëˆ„ë½ ê°ì§€ (Phase 2 í™•ì¥)

    if not missing_ranges:
        logger.info(f"  [{timeframe}] ëˆ„ë½ êµ¬ê°„ ì—†ìŒ")

    return missing_ranges


class MarketDataStorage:
    """ì‹œì¥ ë°ì´í„° SQLite ì €ì¥ì†Œ"""

    def __init__(self, db_path: str = "data/market_data.db"):
        """
        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._initialize_database()

    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™” (ë©€í‹° íƒ€ì„í”„ë ˆì„ ì§€ì›)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë©€í‹° íƒ€ì„í”„ë ˆì„ OHLCV í…Œì´ë¸” ìƒì„±
            for timeframe in ['1m', '1h', '1d']:
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS ohlcv_{timeframe} (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        market TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        open REAL NOT NULL,
                        high REAL NOT NULL,
                        low REAL NOT NULL,
                        close REAL NOT NULL,
                        volume REAL NOT NULL,
                        value REAL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(market, timestamp)
                    )
                """)

            # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ohlcv_data í…Œì´ë¸”ë„ ìœ ì§€ (1ë¶„ë´‰ìœ¼ë¡œ ê°„ì£¼)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ohlcv_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    market TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    open REAL NOT NULL,
                    high REAL NOT NULL,
                    low REAL NOT NULL,
                    close REAL NOT NULL,
                    volume REAL NOT NULL,
                    value REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(market, timestamp)
                )
            """)

            # ì˜¤ë”ë¶ ë°ì´í„° í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS orderbook_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    market TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    asks TEXT NOT NULL,  -- JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    bids TEXT NOT NULL,  -- JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(market, timestamp)
                )
            """)

            # ì²˜ë¦¬ëœ ë°ì´í„° í…Œì´ë¸” (ê¸°ìˆ ì  ì§€í‘œ + íŠ¹ì„±)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS processed_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    market TEXT NOT NULL,
                    timestamp INTEGER NOT NULL,
                    -- ê¸°ìˆ ì  ì§€í‘œ (ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥)
                    sma_5 REAL, sma_20 REAL, sma_60 REAL,
                    ema_12 REAL, ema_26 REAL,
                    rsi_14 REAL,
                    macd REAL, macd_signal REAL, macd_hist REAL,
                    bb_upper REAL, bb_middle REAL, bb_lower REAL,
                    bb_width REAL,
                    atr_14 REAL,
                    obv REAL,
                    stoch_k REAL, stoch_d REAL,
                    volume_sma REAL,
                    price_change_1 REAL, price_change_5 REAL, price_change_20 REAL,
                    -- ì¶”ì¶œëœ íŠ¹ì„± (BLOB)
                    feature_vector BLOB,
                    feature_names TEXT,  -- JSON: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
                    -- ì •ê·œí™” ì •ë³´
                    normalization_method TEXT,
                    normalization_params TEXT,  -- JSON: ì •ê·œí™” íŒŒë¼ë¯¸í„°
                    -- ë©”íƒ€ë°ì´í„°
                    config_hash TEXT NOT NULL,  -- ì„¤ì • í•´ì‹œ (ìºì‹œ ë¬´íš¨í™”)
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(market, timestamp, config_hash)
                )
            """)

            # ì¸ë±ìŠ¤ ìƒì„±
            for timeframe in ['1m', '1h', '1d']:
                cursor.execute(f"""
                    CREATE INDEX IF NOT EXISTS idx_ohlcv_{timeframe}_market_timestamp
                    ON ohlcv_{timeframe}(market, timestamp)
                """)

            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_ohlcv_market_timestamp
                ON ohlcv_data(market, timestamp)
            """)
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_orderbook_market_timestamp
                ON orderbook_data(market, timestamp)
            """)
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_processed_market_timestamp_hash
                ON processed_data(market, timestamp, config_hash)
            """)

            conn.commit()
            self.logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ë©€í‹° íƒ€ì„í”„ë ˆì„ ì§€ì›): {self.db_path}")

    def save_ohlcv_data(self, market: str, data: pd.DataFrame):
        """
        OHLCV ë°ì´í„° ì €ì¥

        Args:
            market: ë§ˆì¼“ ì½”ë“œ (ì˜ˆ: KRW-BTC)
            data: OHLCV ë°ì´í„°í”„ë ˆì„ (ì»¬ëŸ¼: timestamp, open, high, low, close, volume, value)
        """
        with sqlite3.connect(self.db_path) as conn:
            # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            if 'timestamp' not in data.columns:
                data['timestamp'] = data.index

            data = data.copy()
            if pd.api.types.is_datetime64_any_dtype(data['timestamp']):
                data['timestamp'] = data['timestamp'].astype('int64') // 10**9

            # ë°ì´í„° ì‚½ì… (ì¤‘ë³µ ë¬´ì‹œ)
            data['market'] = market
            data[['market', 'timestamp', 'open', 'high', 'low', 'close', 'volume', 'value']].to_sql(
                'ohlcv_data',
                conn,
                if_exists='append',
                index=False,
                method='multi'
            )

            conn.commit()
            self.logger.info(f"{market} OHLCV ë°ì´í„° {len(data)}ê±´ ì €ì¥ ì™„ë£Œ")

    def load_ohlcv_data(
        self,
        market: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        OHLCV ë°ì´í„° ë¡œë“œ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            start_time: ì‹œì‘ ì‹œê°„
            end_time: ì¢…ë£Œ ì‹œê°„
            limit: ìµœëŒ€ ë°ì´í„° ê°œìˆ˜

        Returns:
            OHLCV ë°ì´í„°í”„ë ˆì„
        """
        query = "SELECT timestamp, open, high, low, close, volume, value FROM ohlcv_data WHERE market = ?"
        params = [market]

        if start_time:
            query += " AND timestamp >= ?"
            params.append(int(start_time.timestamp()))

        if end_time:
            query += " AND timestamp <= ?"
            params.append(int(end_time.timestamp()))

        query += " ORDER BY timestamp ASC"

        if limit:
            query += f" LIMIT {limit}"

        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(query, conn, params=params)

        if len(df) == 0:
            self.logger.warning(f"{market} ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame()

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df.set_index('timestamp')

        self.logger.info(f"{market} OHLCV ë°ì´í„° {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ")
        return df

    def get_data_range(self, market: str) -> Tuple[Optional[datetime], Optional[datetime]]:
        """
        ì €ì¥ëœ ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„ ì¡°íšŒ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ

        Returns:
            (ìµœì†Œ ì‹œê°„, ìµœëŒ€ ì‹œê°„)
        """
        query = """
            SELECT MIN(timestamp) as min_time, MAX(timestamp) as max_time
            FROM ohlcv_data WHERE market = ?
        """

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(query, [market])
            result = cursor.fetchone()

        if result and result[0] and result[1]:
            min_time = datetime.fromtimestamp(result[0])
            max_time = datetime.fromtimestamp(result[1])
            return min_time, max_time

        return None, None

    def get_data_count(self, market: str) -> int:
        """
        ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ

        Returns:
            ë°ì´í„° ê°œìˆ˜
        """
        query = "SELECT COUNT(*) FROM ohlcv_data WHERE market = ?"

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(query, [market])
            result = cursor.fetchone()

        return result[0] if result else 0

    def clear_data(self, market: Optional[str] = None):
        """
        ë°ì´í„° ì‚­ì œ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ (Noneì´ë©´ ëª¨ë“  ë°ì´í„° ì‚­ì œ)
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            if market:
                cursor.execute("DELETE FROM ohlcv_data WHERE market = ?", [market])
                cursor.execute("DELETE FROM orderbook_data WHERE market = ?", [market])
                cursor.execute("DELETE FROM processed_data WHERE market = ?", [market])
                self.logger.info(f"{market} ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            else:
                cursor.execute("DELETE FROM ohlcv_data")
                cursor.execute("DELETE FROM orderbook_data")
                cursor.execute("DELETE FROM processed_data")
                self.logger.info("ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

            conn.commit()

    def save_processed_data(
        self,
        market: str,
        data: pd.DataFrame,
        feature_vector: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None,
        normalization_method: str = "robust",
        normalization_params: Optional[Dict] = None,
        config_hash: Optional[str] = None
    ):
        """
        ì²˜ë¦¬ëœ ë°ì´í„°(ê¸°ìˆ ì  ì§€í‘œ + íŠ¹ì„±) ì €ì¥

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            data: ê¸°ìˆ ì  ì§€í‘œê°€ í¬í•¨ëœ DataFrame
            feature_vector: ì¶”ì¶œëœ íŠ¹ì„± ë²¡í„° (shape: [n_samples, n_features])
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            normalization_method: ì •ê·œí™” ë°©ë²•
            normalization_params: ì •ê·œí™” íŒŒë¼ë¯¸í„°
            config_hash: ì„¤ì • í•´ì‹œ
        """
        if config_hash is None:
            config_hash = self._generate_config_hash(
                normalization_method, normalization_params
            )

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë°ì´í„° ì¤€ë¹„
            data_to_save = data.copy()

            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
            if 'timestamp' not in data_to_save.columns:
                data_to_save['timestamp'] = data_to_save.index

            if pd.api.types.is_datetime64_any_dtype(data_to_save['timestamp']):
                data_to_save['timestamp'] = (
                    data_to_save['timestamp'].astype('int64') // 10**9
                )

            # ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼ (í…Œì´ë¸”ì— ì •ì˜ëœ ê²ƒ)
            indicator_columns = [
                'sma_5', 'sma_20', 'sma_60',
                'ema_12', 'ema_26',
                'rsi_14',
                'macd', 'macd_signal', 'macd_hist',
                'bb_upper', 'bb_middle', 'bb_lower', 'bb_width',
                'atr_14', 'obv',
                'stoch_k', 'stoch_d',
                'volume_sma',
                'price_change_1', 'price_change_5', 'price_change_20'
            ]

            # ë°ì´í„° í–‰ë³„ë¡œ ì €ì¥
            for idx, row in data_to_save.iterrows():
                timestamp = int(row['timestamp'])

                # ê¸°ìˆ ì  ì§€í‘œ ê°’ ì¶”ì¶œ
                indicator_values = {}
                for col in indicator_columns:
                    if col in data_to_save.columns:
                        val = row[col]
                        indicator_values[col] = (
                            float(val) if pd.notna(val) else None
                        )
                    else:
                        indicator_values[col] = None

                # íŠ¹ì„± ë²¡í„° ì§ë ¬í™”
                feature_blob = None
                if feature_vector is not None and len(feature_vector) > idx:
                    feature_blob = pickle.dumps(feature_vector[idx])

                # íŠ¹ì„± ì´ë¦„ JSON ì§ë ¬í™”
                feature_names_json = (
                    json.dumps(feature_names) if feature_names else None
                )

                # ì •ê·œí™” íŒŒë¼ë¯¸í„° JSON ì§ë ¬í™”
                norm_params_json = (
                    json.dumps(normalization_params) if normalization_params else None
                )

                # INSERT OR REPLACE
                cursor.execute("""
                    INSERT OR REPLACE INTO processed_data (
                        market, timestamp,
                        sma_5, sma_20, sma_60,
                        ema_12, ema_26,
                        rsi_14,
                        macd, macd_signal, macd_hist,
                        bb_upper, bb_middle, bb_lower, bb_width,
                        atr_14, obv,
                        stoch_k, stoch_d,
                        volume_sma,
                        price_change_1, price_change_5, price_change_20,
                        feature_vector, feature_names,
                        normalization_method, normalization_params,
                        config_hash
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, [
                    market, timestamp,
                    indicator_values['sma_5'], indicator_values['sma_20'], indicator_values['sma_60'],
                    indicator_values['ema_12'], indicator_values['ema_26'],
                    indicator_values['rsi_14'],
                    indicator_values['macd'], indicator_values['macd_signal'], indicator_values['macd_hist'],
                    indicator_values['bb_upper'], indicator_values['bb_middle'], indicator_values['bb_lower'], indicator_values['bb_width'],
                    indicator_values['atr_14'], indicator_values['obv'],
                    indicator_values['stoch_k'], indicator_values['stoch_d'],
                    indicator_values['volume_sma'],
                    indicator_values['price_change_1'], indicator_values['price_change_5'], indicator_values['price_change_20'],
                    feature_blob, feature_names_json,
                    normalization_method, norm_params_json,
                    config_hash
                ])

            conn.commit()
            self.logger.info(f"{market} ì²˜ë¦¬ëœ ë°ì´í„° {len(data_to_save)}ê±´ ì €ì¥ ì™„ë£Œ (hash: {config_hash[:8]})")

    def load_processed_data(
        self,
        market: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        config_hash: Optional[str] = None,
        limit: Optional[int] = None
    ) -> Optional[pd.DataFrame]:
        """
        ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            start_time: ì‹œì‘ ì‹œê°„
            end_time: ì¢…ë£Œ ì‹œê°„
            config_hash: ì„¤ì • í•´ì‹œ (Noneì´ë©´ ìµœì‹  ê²ƒ ì‚¬ìš©)
            limit: ìµœëŒ€ ë°ì´í„° ê°œìˆ˜

        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„° DataFrame (feature_vector í¬í•¨)
        """
        query = """
            SELECT timestamp,
                   sma_5, sma_20, sma_60,
                   ema_12, ema_26,
                   rsi_14,
                   macd, macd_signal, macd_hist,
                   bb_upper, bb_middle, bb_lower, bb_width,
                   atr_14, obv,
                   stoch_k, stoch_d,
                   volume_sma,
                   price_change_1, price_change_5, price_change_20,
                   feature_vector, feature_names,
                   normalization_method, normalization_params,
                   config_hash
            FROM processed_data WHERE market = ?
        """
        params = [market]

        if config_hash:
            query += " AND config_hash = ?"
            params.append(config_hash)

        if start_time:
            query += " AND timestamp >= ?"
            params.append(int(start_time.timestamp()))

        if end_time:
            query += " AND timestamp <= ?"
            params.append(int(end_time.timestamp()))

        query += " ORDER BY timestamp ASC"

        if limit:
            query += f" LIMIT {limit}"

        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(query, conn, params=params)

        if len(df) == 0:
            self.logger.warning(f"{market} ì²˜ë¦¬ëœ ë°ì´í„° ì—†ìŒ")
            return None

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df.set_index('timestamp')

        # feature_vector ì—­ì§ë ¬í™”
        if 'feature_vector' in df.columns:
            df['feature_vector'] = df['feature_vector'].apply(
                lambda x: pickle.loads(x) if x is not None else None
            )

        # feature_names ì—­ì§ë ¬í™”
        if 'feature_names' in df.columns:
            df['feature_names'] = df['feature_names'].apply(
                lambda x: json.loads(x) if x is not None else None
            )

        # normalization_params ì—­ì§ë ¬í™”
        if 'normalization_params' in df.columns:
            df['normalization_params'] = df['normalization_params'].apply(
                lambda x: json.loads(x) if x is not None else None
            )

        self.logger.info(
            f"{market} ì²˜ë¦¬ëœ ë°ì´í„° {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ "
            f"(hash: {df['config_hash'].iloc[0][:8] if len(df) > 0 else 'N/A'})"
        )
        return df

    def _generate_config_hash(
        self,
        normalization_method: str,
        normalization_params: Optional[Dict] = None
    ) -> str:
        """ì„¤ì • ê¸°ë°˜ í•´ì‹œ ìƒì„± (ìºì‹œ ë¬´íš¨í™”ìš©)"""
        config_str = f"{normalization_method}_{normalization_params}"
        return hashlib.md5(config_str.encode()).hexdigest()

    # ========================================
    # ë©€í‹° íƒ€ì„í”„ë ˆì„ ë©”ì„œë“œ (1m, 1h, 1d)
    # ========================================

    def save_ohlcv_data_by_timeframe(
        self,
        market: str,
        data: pd.DataFrame,
        timeframe: str
    ):
        """
        íƒ€ì„í”„ë ˆì„ë³„ OHLCV ë°ì´í„° ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ì •ê·œí™” + ì¤‘ë³µ ë°©ì§€)

        Args:
            market: ë§ˆì¼“ ì½”ë“œ (ì˜ˆ: KRW-BTC)
            data: OHLCV ë°ì´í„°í”„ë ˆì„ (ì»¬ëŸ¼: timestamp, open, high, low, close, volume, value)
            timeframe: íƒ€ì„í”„ë ˆì„ ('1m', '1h', '1d')
        """
        if timeframe not in ['1m', '1h', '1d']:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¤€ë¹„
            if 'timestamp' not in data.columns:
                data['timestamp'] = data.index

            data = data.copy()

            # datetime â†’ ì •ê·œí™” â†’ Unix timestamp (int)
            if pd.api.types.is_datetime64_any_dtype(data['timestamp']):
                # 1. íƒ€ì„ìŠ¤íƒ¬í”„ ì •ê·œí™” (ì´ˆ/ë°€ë¦¬ì´ˆ ì œê±°)
                data['timestamp'] = data['timestamp'].apply(lambda x: align_timestamp(x, timeframe))
                # 2. Unix timestampë¡œ ë³€í™˜
                data['timestamp'] = data['timestamp'].astype('int64') // 10**9

            # INSERT OR REPLACE ë°©ì‹ìœ¼ë¡œ ì €ì¥ (ì¤‘ë³µ ìë™ ì²˜ë¦¬)
            data['market'] = market

            for _, row in data.iterrows():
                cursor.execute(
                    f"""
                    INSERT OR REPLACE INTO ohlcv_{timeframe}
                    (market, timestamp, open, high, low, close, volume, value)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    [
                        market,
                        int(row['timestamp']),
                        float(row['open']),
                        float(row['high']),
                        float(row['low']),
                        float(row['close']),
                        float(row['volume']),
                        float(row.get('value', 0))
                    ]
                )

            conn.commit()
            self.logger.info(f"{market} OHLCV ë°ì´í„° {len(data)}ê±´ ì €ì¥ ì™„ë£Œ (íƒ€ì„í”„ë ˆì„: {timeframe})")

    def load_ohlcv_data_by_timeframe(
        self,
        market: str,
        timeframe: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        íƒ€ì„í”„ë ˆì„ë³„ OHLCV ë°ì´í„° ë¡œë“œ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            timeframe: íƒ€ì„í”„ë ˆì„ ('1m', '1h', '1d')
            start_time: ì‹œì‘ ì‹œê°„
            end_time: ì¢…ë£Œ ì‹œê°„
            limit: ìµœëŒ€ ë°ì´í„° ê°œìˆ˜

        Returns:
            OHLCV ë°ì´í„°í”„ë ˆì„
        """
        if timeframe not in ['1m', '1h', '1d']:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")

        query = f"SELECT timestamp, open, high, low, close, volume, value FROM ohlcv_{timeframe} WHERE market = ?"
        params = [market]

        if start_time:
            query += " AND timestamp >= ?"
            params.append(int(start_time.timestamp()))

        if end_time:
            query += " AND timestamp <= ?"
            params.append(int(end_time.timestamp()))

        query += " ORDER BY timestamp ASC"

        if limit:
            query += f" LIMIT {limit}"

        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(query, conn, params=params)

        if len(df) == 0:
            self.logger.warning(f"{market} ë°ì´í„° ì—†ìŒ (íƒ€ì„í”„ë ˆì„: {timeframe})")
            return pd.DataFrame()

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df.set_index('timestamp')

        self.logger.info(f"{market} OHLCV ë°ì´í„° {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ (íƒ€ì„í”„ë ˆì„: {timeframe})")
        return df

    def get_data_range_by_timeframe(
        self,
        market: str,
        timeframe: str
    ) -> Tuple[Optional[datetime], Optional[datetime]]:
        """
        íƒ€ì„í”„ë ˆì„ë³„ ë°ì´í„° ì‹œê°„ ë²”ìœ„ ì¡°íšŒ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            timeframe: íƒ€ì„í”„ë ˆì„ ('1m', '1h', '1d')

        Returns:
            (ìµœì†Œ ì‹œê°„, ìµœëŒ€ ì‹œê°„)
        """
        if timeframe not in ['1m', '1h', '1d']:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")

        query = f"""
            SELECT MIN(timestamp) as min_time, MAX(timestamp) as max_time
            FROM ohlcv_{timeframe} WHERE market = ?
        """

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(query, [market])
            result = cursor.fetchone()

        if result and result[0] and result[1]:
            min_time = datetime.fromtimestamp(result[0])
            max_time = datetime.fromtimestamp(result[1])
            return min_time, max_time

        return None, None

    def get_data_count_by_timeframe(
        self,
        market: str,
        timeframe: str
    ) -> int:
        """
        íƒ€ì„í”„ë ˆì„ë³„ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ

        Args:
            market: ë§ˆì¼“ ì½”ë“œ
            timeframe: íƒ€ì„í”„ë ˆì„ ('1m', '1h', '1d')

        Returns:
            ë°ì´í„° ê°œìˆ˜
        """
        if timeframe not in ['1m', '1h', '1d']:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")

        query = f"SELECT COUNT(*) FROM ohlcv_{timeframe} WHERE market = ?"

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(query, [market])
            result = cursor.fetchone()

        return result[0] if result else 0


def collect_multi_timeframe_data(
    market: str,
    start_time: datetime,
    end_time: Optional[datetime] = None,
    minute_candles_count: Optional[int] = None,
    hourly_lookback_count: int = 24,
    daily_lookback_count: int = 30,
    db_path: str = "data/market_data.db",
    show_progress: bool = True
):
    """
    ë©€í‹° íƒ€ì„í”„ë ˆì„ ë°ì´í„° ìˆ˜ì§‘ (í†µí•© í•¨ìˆ˜)

    Args:
        market: ë§ˆì¼“ ì½”ë“œ (ì˜ˆ: KRW-BTC)
        start_time: ì‹œì‘ ì‹œê°„
        end_time: ì¢…ë£Œ ì‹œê°„ (minute_candles_countì™€ ë‘˜ ì¤‘ í•˜ë‚˜ í•„ìˆ˜)
        minute_candles_count: 1ë¶„ë´‰ ê°œìˆ˜ (end_time ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥)
        hourly_lookback_count: ì‹œì‘ ì´ì „ ì‹œê°„ë´‰ ê°œìˆ˜
        daily_lookback_count: ì‹œì‘ ì´ì „ ì¼ë´‰ ê°œìˆ˜
        db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€

    Examples:
        # ë°©ë²• 1: end_time ì§€ì •
        collect_multi_timeframe_data(
            market="KRW-BTC",
            start_time=datetime(2025, 10, 1),
            end_time=datetime(2025, 10, 10)
        )

        # ë°©ë²• 2: minute_candles_count ì§€ì •
        collect_multi_timeframe_data(
            market="KRW-BTC",
            start_time=datetime(2025, 10, 1),
            minute_candles_count=1440  # 1ì¼ì¹˜
        )
    """
    from .market_data import UpbitDataCollector

    try:
        from tqdm import tqdm
        HAS_TQDM = True
    except ImportError:
        HAS_TQDM = False
        logging.warning("tqdmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§„í–‰ë¥  í‘œì‹œê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # 1. end_time ê³„ì‚°
    if end_time is None and minute_candles_count is not None:
        end_time = start_time + timedelta(minutes=minute_candles_count)
    elif end_time is None:
        raise ValueError("end_time ë˜ëŠ” minute_candles_count ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    # 2. íƒ€ì„í”„ë ˆì„ë³„ ëª©í‘œ ë²”ìœ„ ê³„ì‚° + ì •ê·œí™”
    targets = {
        '1m': (
            align_timestamp(start_time, '1m'),
            align_timestamp(end_time, '1m')
        ),
        '1h': (
            align_timestamp(start_time - timedelta(hours=hourly_lookback_count), '1h'),
            align_timestamp(end_time, '1h')
        ),
        '1d': (
            align_timestamp(start_time - timedelta(days=daily_lookback_count), '1d'),
            align_timestamp(end_time, '1d')
        )
    }

    logger.info(f"[ë©€í‹° íƒ€ì„í”„ë ˆì„ ìˆ˜ì§‘ ì‹œì‘] {market}")
    logger.info(f"  ê¸°ê°„: {start_time} ~ {end_time}")
    logger.info(f"  ì¶”ê°€ lookback: ì‹œê°„ë´‰ {hourly_lookback_count}ê°œ, ì¼ë´‰ {daily_lookback_count}ê°œ")

    # 3. ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
    storage = MarketDataStorage(db_path)
    collector = UpbitDataCollector(market)

    for timeframe, (target_start, target_end) in targets.items():
        logger.info(f"[{timeframe}] ëª©í‘œ: {target_start} ~ {target_end}")

        # ëˆ„ë½ ë²”ìœ„ ê³„ì‚°
        missing_ranges = get_missing_ranges(storage, market, timeframe, target_start, target_end)

        if not missing_ranges:
            logger.info(f"  âœ… ì´ë¯¸ ëª¨ë“  ë°ì´í„° ì¡´ì¬ (ìˆ˜ì§‘ ìŠ¤í‚µ)")
            continue

        # ëˆ„ë½ êµ¬ê°„ ìˆ˜ì§‘
        for miss_start, miss_end in missing_ranges:
            logger.info(f"  ğŸ“¥ ìˆ˜ì§‘ ì¤‘: {miss_start} ~ {miss_end}")

            # í•„ìš” ë´‰ ê°œìˆ˜ ê³„ì‚°
            unit_minutes = {'1m': 1, '1h': 60, '1d': 1440}[timeframe]
            total_candles = int((miss_end - miss_start).total_seconds() / (unit_minutes * 60))

            # 200ê°œì”© ë¶„í•  ìˆ˜ì§‘
            MAX_PER_REQUEST = UpbitDataCollector.MAX_CANDLES_PER_REQUEST
            num_requests = (total_candles + MAX_PER_REQUEST - 1) // MAX_PER_REQUEST

            all_data = []
            pbar = None

            if show_progress and HAS_TQDM:
                pbar = tqdm(total=num_requests, desc=f"  [{timeframe}] ìˆ˜ì§‘", leave=False)

            for i in range(num_requests):
                count = min(MAX_PER_REQUEST, total_candles - i * MAX_PER_REQUEST)

                # API ìš”ì²­ (Rate Limit ìë™ ì²˜ë¦¬)
                data = collector.get_historical_data(count=count, unit=unit_minutes)

                if data is not None and not data.empty:
                    all_data.append(data)

                if pbar:
                    pbar.update(1)

            if pbar:
                pbar.close()

            # ë°ì´í„° ë³‘í•© ë° ì €ì¥
            if all_data:
                merged_data = pd.concat(all_data).sort_index()

                # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ê·œí™”
                merged_data.index = merged_data.index.map(lambda x: align_timestamp(x, timeframe))

                # ì¤‘ë³µ ì œê±° í›„ ì €ì¥
                merged_data = merged_data[~merged_data.index.duplicated(keep='first')]
                storage.save_ohlcv_data_by_timeframe(market, merged_data, timeframe)
                logger.info(f"  âœ… {len(merged_data)}ê°œ ì €ì¥ ì™„ë£Œ")
            else:
                logger.error(f"  âŒ ìˆ˜ì§‘ ì‹¤íŒ¨")

    # 4. ìµœì¢… ê²°ê³¼
    logger.info("\n[ìˆ˜ì§‘ ì™„ë£Œ]")
    for tf in ['1m', '1h', '1d']:
        count = storage.get_data_count_by_timeframe(market, tf)
        data_range = storage.get_data_range_by_timeframe(market, tf)
        logger.info(f"  {tf}: {count}ê±´, ë²”ìœ„: {data_range[0]} ~ {data_range[1]}")


def collect_and_store_data(
    market: str = "KRW-BTC",
    count: int = 500,
    unit: int = 1,
    db_path: str = "data/market_data.db"
):
    """
    Upbitì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  SQLiteì— ì €ì¥

    Args:
        market: ë§ˆì¼“ ì½”ë“œ
        count: ìˆ˜ì§‘í•  ìº”ë“¤ ê°œìˆ˜
        unit: ìº”ë“¤ ë‹¨ìœ„ (ë¶„)
        db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
    """
    from .market_data import UpbitDataCollector

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # ë°ì´í„° ìˆ˜ì§‘
    logger.info(f"{market} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (count={count}, unit={unit}ë¶„)")
    collector = UpbitDataCollector(market)
    data = collector.get_historical_data(count=count, unit=unit)

    if data is None or len(data) == 0:
        logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return

    # ë°ì´í„° ì €ì¥
    storage = MarketDataStorage(db_path)
    storage.save_ohlcv_data(market, data)

    # ì €ì¥ ê²°ê³¼ í™•ì¸
    data_count = storage.get_data_count(market)
    data_range = storage.get_data_range(market)
    logger.info(f"ì €ì¥ ì™„ë£Œ: {data_count}ê±´, ë²”ìœ„: {data_range[0]} ~ {data_range[1]}")


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ
    collect_and_store_data(market="KRW-BTC", count=1000, unit=1)
